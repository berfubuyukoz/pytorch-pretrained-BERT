# -*- coding: utf-8 -*-
"""bert_for_sequence_classification-gpu-train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Ni4YOT39pYc1hSzpnw0YTSm6EZ8glvd

References: https://www.kaggle.com/bbqlp33/bert-huggingface-pytorch-pretrained-bert

**NOTE:**

- Uses customized pytorch BERT library "pytorch_pretrained_BERT" pulled from my github repo. It is user's responsibility to use the right version of it.

**IMPROVE:**

- Imbalance in data could be handles better. (Large portion belongs to positive class (protest))
- Bert's uncased tokenizer is being used. It is most probably better than cased, as with uncased tokenizer we can normalize capitalized forms of a word into lower case.
"""

'''This is the duplicate of its ipynb version - of course at the time of the copying operation. I do not keep this file up-to-date with referred ipynb.'''
'''I added this python code to debug an issue when calling BertForSequenceClassification. But I got import errors and returned to ipynb.'''

TASK_DATA_DIR = "../my_data/"

#problem arguments
CLASSIFICATION_PROBLEM = 'binary'
TARGET_NAMES = ['protest', 'non-protest']

#model arguments
BERT_MODEL_NAME = 'bert-base-uncased'
DO_LOWER_CASE = True if '-cased' not in BERT_MODEL_NAME else False
SETTING = 'neutral'
MAX_SEQ_LENGTH = 128


TRAIN_BATCH_SIZE = 32
NUM_TRAIN_EPOCHS = 3.0
LEARNING_RATE = 2e-5
SEED = 42
LOCAL_RANK = -1
#NO_CUDA = False
NO_CUDA = True
OUTPUT_MODE = "classification"
TASK_NAME = "long text classification" #Document classification

#data argument
TASK_DATA_DIR_EDITED = TASK_DATA_DIR
data_file_name = '20190221_Reuters_codeChina_SVM_RF_Bert_preds_Adjudication_base_edited.json'
#TRAIN_FILE_PATH = '/Users/buyukozb/git/berfu/thesis/data/20190221_Reuters_codeChina_SVM_RF_Bert_preds_Adjudication_base_edited.json'
TRAIN_FILE_PATH = TASK_DATA_DIR_EDITED + data_file_name
DATA_COLUMN = 'text'
LABEL_COLUMN = 'label'
LABEL_LIST = [0, 1]
NUM_LABELS = len(LABEL_LIST)
OUTPUT_DIR = '../my_outputs/bert_out/'
OUTPUT_TRAIN_FILE = 'train_results.txt'
HYPERPARAMS_FILE = 'model_hyperparams.txt' #what distinguishes this model from others.

RANDOM_STATE = 13

# Save model hyperparameters info
hyperparams = {"TASK_NAME": TASK_NAME,
 "SETTING": SETTING,
 "TRAIN_FILE_PATH": TRAIN_FILE_PATH,
 "NUM_LABELS": NUM_LABELS,
 "MODEL_NAME": BERT_MODEL_NAME, 
 "MAX_SEQ_LENGTH": MAX_SEQ_LENGTH,
 "TRAIN_BATCH_SIZE": TRAIN_BATCH_SIZE,
 "NUM_TRAIN_EPOCHS": NUM_TRAIN_EPOCHS,
 "LEARNING_RATE": LEARNING_RATE,
 "OPTIMIZER": "Adam",
 "LOSS": "Cross-entropy loss"
}

import os
if os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):
    raise ValueError("Output directory ({}) already exists and is not empty.".format(OUTPUT_DIR))
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

output_hyperparams_file = os.path.join(OUTPUT_DIR, HYPERPARAMS_FILE)

with open(output_hyperparams_file, "w") as writer:
  for key in hyperparams.keys():
    writer.write("%s = %s\n" % (key, str(hyperparams[key])))

from sys import path as pylib #im naming it as pylib so that we won't get confused between os.path and sys.path
pylib += [os.path.abspath('/Users/buyukozb/git/berfu')]
from pytorch_pretrained_BERT.examples.my_run_classifier import InputExample
from pytorch_pretrained_BERT.examples.my_run_classifier import convert_examples_to_features
from pytorch_pretrained_BERT.pytorch_pretrained_bert import BertTokenizer
from pytorch_pretrained_BERT.pytorch_pretrained_bert.modeling import BertConfig, BertModel, BertForSequenceClassification
from pytorch_pretrained_BERT.pytorch_pretrained_bert.optimization import BertAdam
from pytorch_pretrained_BERT.pytorch_pretrained_bert.file_utils import WEIGHTS_NAME, CONFIG_NAME

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit
from sklearn.metrics import matthews_corrcoef, f1_score, accuracy_score, balanced_accuracy_score
from tqdm import tqdm, trange
import pickle
import gc
from keras.layers import Dense, Input, LSTM, Embedding, SpatialDropout1D, Dropout, Activation, Bidirectional, GlobalMaxPool1D
from keras.models import Model
import torch
from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,
                              TensorDataset)
from torch.nn import CrossEntropyLoss

if TRAIN_FILE_PATH.endswith('json'):
    df = pd.read_json(TRAIN_FILE_PATH, orient='records')
    tlist = df['text'].tolist()
    llist = df['label'].tolist()
elif TRAIN_FILE_PATH.endswith('xlsx') or TRAIN_FILE_PATH.endswith('xls'):
    import xlrd
    df = pd.read_excel(TRAIN_FILE_PATH, index_col=0, dtype={'text': str})
    tlist = df['text']
    llist = df['label']

rs = StratifiedShuffleSplit(n_splits=1, test_size=.20, random_state=RANDOM_STATE)
splitted = list(rs.split(tlist,llist))
train_indices = splitted[0][0].tolist()

train_x = [t for i,t in enumerate(tlist) if i in train_indices]
train_y = [t for i,t in enumerate(llist) if i in train_indices]

train_num_protest = [i for i,e in enumerate(train_y) if e == 1]
train_num_plannedprotest = [i for i,e in enumerate(train_y) if e == 2]
train_num_nonprotest = [i for i,e in enumerate(train_y) if e == 0]

print('Train num protest', len(train_num_protest) )
print('Train num planned-protest', len(train_num_plannedprotest))
print('Train num nonprotest', len(train_num_nonprotest))

print('Num train samples: ' + str(len(train_y)))

if CLASSIFICATION_PROBLEM == 'binary':
    train_y_binary = [0 if t==2 else t for t in train_y]
    train_planned = [t for t in train_y if t == 2]
    train_nonprotest_before = [t for t in train_y if t == 0]
    train_nonprotest_after = [t for t in train_y_binary if t == 0]
    assert len(train_nonprotest_after) == len(train_nonprotest_before) + len(train_planned)

train_num_protest_new = [i for i,e in enumerate(train_y_binary) if e == 1]
train_num_nonprotest_new = [i for i,e in enumerate(train_y_binary) if e == 0]

print('Train num protest after binarizing', len(train_num_protest_new) )
print('Train num nonprotest after binarizing', len(train_num_nonprotest_new))

train_zip = list(zip(train_x, train_y_binary))  
train = pd.DataFrame(train_zip, columns = [DATA_COLUMN, LABEL_COLUMN])

test = pd.DataFrame(test_zip, columns = [DATA_COLUMN, LABEL_COLUMN])

#train_examples = processor.get_train_examples(TASK_DATA_DIR)
# customize get_train_examples:
# Use the InputExample class from BERT's run_classifier code to create examples from the data

# Train examples is the formatted version of train texts. Each text (news text) is converted to an InputExample.
# No preprocessing is done at this step. Text is raw bulk text.
# text_a contains a text input's whole text data.
# We do not have text_b because our task is single text classification, not something to do with interrelations of texts.
# label is the class label of text.
train_examples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example
                                                    text_a = x[DATA_COLUMN], 
                                                    text_b = None, 
                                                    label = x[LABEL_COLUMN]), axis = 1)

print('Num train examples: ' + str(len(train_examples)))

print('An input example - text: ' + str(train_examples[0].label))
print('An input example - text: ' + train_examples[0].text_a)

num_protest = [i for i,e in enumerate(train_examples) if e.label == 1]
num_nonprotest = [i for i,e in enumerate(train_examples) if e.label == 0]

"""Imbalanced data. Large portion belongs to positive class (protest)."""

print('Train protest num samples:', len(num_protest))
print('Train nonprotest num samples:', len(num_nonprotest))
print('Train protest/total sample ratio:', len(num_protest)/len(train_examples))

"""Bert tokenizer default behavior:

- basic tokenization:

    - split by whitespace
    
    - do_lower_case (if tokenizer is imported from a pretrained bert model, this variable is set based on the preferred pretrained model. If pretrained model is bert-base-uncased, lower is set to True.
    
    - punctuation splitting - not removal! This means, certain punctuation marks are treated as individual tokens.
    
- wordpiece tokenization
"""

tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)

VOCAB_SIZE = len(tokenizer.vocab)

"""Convert_examples_to_features does:

- For each text
    - Tokenize (text is a list of tokens)
    - Add [CLS] and [SEP] tokens at both ends of the tokens list.
    - CLS and SEP takes space from MAX_SEQ_LEN 
    
- Returns list of InputFeatures objects as such:

    - InputFeatures:
        - input_ids=input_ids (each token's id (int) is retrieved from pretrained bert model's vocab txt)
        - input_mask=input_mask (add 0 (int) as mask to texts shorter then MAX_SEQ_LEN)
        - segment_ids=segment_ids (all are 0 (int) because there is only one segment. There is no text_b.)
        - label_id=label_id (One id (int) for text's class label. Id is a label's enumeration. In my case labels are already integer: nonprotest: 0, protest: 1)
"""

train_features = convert_examples_to_features(
    train_examples, LABEL_LIST, MAX_SEQ_LENGTH, tokenizer, output_mode=OUTPUT_MODE)
    
#tokenized_sent = tokenizer.tokenize(s)
#indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_sent)

input_ids = [tf.input_ids for tf in train_features]
input_masks = [tf.input_mask for tf in train_features]
segment_ids = [tf.segment_ids for tf in train_features]
label_ids = [tf.label_id for tf in train_features]


"""- **x_train_ids** is a list of lists, in which, each list represent an input text; and each list are
composed of token ids retrieved from the vocabulary of BERT. 

- If a text is smaller than MAX_SEQ_LENGTH, it is **padded with zeros** for each token (with bert, it is subword).
- Below cell is unnecessary because all it does is padding input ids; which are already padded in BertTokenizer.
"""

'''
x_train_ids = np.zeros((train.shape[0],MAX_SEQ_LENGTH),dtype=np.int)
for i,tf in enumerate(train_features):
    input_ids = tf.input_ids[:MAX_SEQ_LENGTH]
    inp_len = len(input_ids)
    x_train_ids[i,:inp_len] = np.array(input_ids)
    
x_text_ids = np.zeros((test.shape[0],MAX_SEQ_LENGTH),dtype=np.int)

for i,tf in enumerate(test_features):

    input_ids = tf.input_ids[:MAX_SEQ_LENGTH]
    inp_len = len(input_ids)
    x_text_ids[i,:inp_len] = np.array(input_ids)

'''
gc.collect()

if LOCAL_RANK == -1 or NO_CUDA:
    device = torch.device("cuda" if torch.cuda.is_available() and not NO_CUDA else "cpu")
    n_gpu = torch.cuda.device_count()
else:
    torch.cuda.set_device(LOCAL_RANK)
    device = torch.device("cuda", LOCAL_RANK)
    n_gpu = 1
    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
    torch.distributed.init_process_group(backend='nccl')

#n_gpu

'''
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if n_gpu > 0:
  torch.cuda.manual_seed_all(SEED)
'''

model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME,num_labels=NUM_LABELS)
model.to(device)
#if n_gpu > 1: 
   # model = torch.nn.DataParallel(model)

model_params = list(model.named_parameters())
model_params_names = [p[0] for p in model_params]
for n in model_params_names:
  print(n)

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
# Below weight_decay of all parameters except the ones in no_decay list is set to 0.01. No_decay list weight decay is set to 0.
optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

num_train_optimization_steps = int(
            len(train_examples) / TRAIN_BATCH_SIZE) * NUM_TRAIN_EPOCHS

optimizer = BertAdam(optimizer_grouped_parameters,
                            lr=LEARNING_RATE,
                            t_total=num_train_optimization_steps)

#input_ids = torch.LongTensor(x_train_ids)
all_input_ids = torch.LongTensor(input_ids)
all_input_mask = torch.LongTensor(input_masks)
all_segment_ids = torch.LongTensor(segment_ids)
all_label_ids = torch.LongTensor(label_ids)

train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)
train_sampler = SequentialSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)

len(list(train_dataloader)[1])

model.train()

gc.collect() #This apparently may temporarily solve out of memory issue of Colab.

global_step = 0
nb_tr_steps = 0
tr_loss = 0
for _ in trange(int(NUM_TRAIN_EPOCHS), desc="Epoch"):
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
        batch = tuple(t.to(device) for t in batch)
        input_ids, input_mask, segment_ids, label_ids = batch

        # define a new function to compute loss values for both output_modes
        # I deliberately put out dropout and classifier lines below from bertforsequenceclassification class in order to have more control over training.
        # So that in future I could arrange training process to use any encoded layer.
        # I might need to do that to be able to compare BERT with other contextual representations in a feature-based manner. 
        # I might even use the same classification steps for ELMO and OpenAI Transformer.
        # I actually have to arrange an identical environment for a proper comparison.
        encoded_layers, pooled_output = model(input_ids, segment_ids, input_mask)
        pooled_output = model.dropout(pooled_output)
        logits = model.classifier(pooled_output)

        loss_fct = CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, NUM_LABELS), label_ids.view(-1))

        if n_gpu > 1:
            loss = loss.mean()  # mean() to average on multi-gpu.

        loss.backward()

        tr_loss += loss.item()
        nb_tr_examples += input_ids.size(0)
        nb_tr_steps += 1
        
        optimizer.step()
        optimizer.zero_grad()
        global_step += 1

# Save a trained model, configuration and tokenizer
model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self

# If we save using the predefined names, we can load using `from_pretrained`
output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)
output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)

torch.save(model_to_save.state_dict(), output_model_file)
model_to_save.config.to_json_file(output_config_file)
tokenizer.save_vocabulary(OUTPUT_DIR)
model.to(device)